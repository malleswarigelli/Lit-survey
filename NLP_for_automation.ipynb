{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04018e5a",
   "metadata": {},
   "source": [
    "# Built ML program for Abstract classificication and Automaton/model deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c65f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malli.gelli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We successfully opened the json file on azure\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 0. 1.]]\n",
      "['medium']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n",
      "prob_values: [[0. 1. 0.]]\n",
      "['low']\n",
      "prob_values: [[1. 0. 0.]]\n",
      "['high']\n"
     ]
    }
   ],
   "source": [
    "#Import numerical libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Text preprocessing libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "#Import resampling and modeling algorithms\n",
    "from sklearn.utils import resample # for Bootstrap sampling\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#KFold CV\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Access new data from Azure data store\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# Researchers experiment data\n",
    "df= pd.read_excel ('Input1_.xlsx')\n",
    "# rename colums and print head\n",
    "df.rename(columns={'Title ': 'Title', 'rank of relevance (1=high, 2=medium, 3=low)':'Class'}, inplace=True)\n",
    "#slice needed columns\n",
    "df=df[['Title','Abstract','Class','Rank']]\n",
    "# drop rows with no abstract, Class\n",
    "df= df[df['Abstract'].notna()]\n",
    "df=df[df['Class'].notna()]\n",
    "\n",
    "\n",
    "# Text preprocessing: stemming Abstracts\n",
    "stemmer1 = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "df['stem_text'] = df['Abstract'].apply(\n",
    "    lambda x: \" \".join([stemmer1.stem(i) \n",
    "                        for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "\n",
    "# lemmatize abstracts\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "df['lemma_text'] = df['Abstract'].apply(\n",
    "    lambda x: \" \".join([lemmatizer.lemmatize(i) \n",
    "                        for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "\n",
    "## Slice feature and target variables, convert to arrays\n",
    "data=df[['lemma_text','Rank']]\n",
    "X= data['lemma_text'].values\n",
    "y = data['Rank'].values\n",
    "\n",
    "## Train test split data\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "## Transform words into vectors with TF-IDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tf = vectorizer_tfidf.fit_transform(X_train.ravel())\n",
    "X_train_tf = vectorizer_tfidf.transform(X_train.ravel())\n",
    "    \n",
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf = vectorizer_tfidf.transform(X_test.ravel())\n",
    "\n",
    "pickle.dump(vectorizer_tfidf, open('vectorizer.pkl','wb'))\n",
    "\n",
    "# Supervised Machine Learning Classification models\n",
    "\n",
    "# Fit naive bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train_tf, y_train)    \n",
    "#predicted train data\n",
    "y_pred_train_NB = NB.predict(X_train_tf)\n",
    "#print('Ranks of y_train\\n:', y_train)\n",
    "#print('-----------------------------------')\n",
    "#print('NB classifier Training set predictions:\\n',y_pred_train_NB)\n",
    "#print('Training set accuracy:', accuracy_score(y_train, y_pred_train_NB))\n",
    "y_pred_test_NB = NB.predict(X_test_tf)\n",
    "#print('Ranks of y_test\\n:', y_test)\n",
    "#print('-----------------------------------')\n",
    "#print('NB classifier Test set predictions:\\n',y_pred_test_NB)\n",
    "naive_score = accuracy_score(y_test, y_pred_test_NB)\n",
    "#print('Test set accuracy:', np.round(naive_score,3))\n",
    "#print(metrics.classification_report(y_test, y_pred_test_NB, target_names=['high', 'medium','low']))   \n",
    "#print(\"Confusion matrix:\")\n",
    "#print(metrics.confusion_matrix(y_test, y_pred_test_NB))\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train_tf, y_train)\n",
    "#predicted train data\n",
    "y_pred_train_LR = LR.predict(X_train_tf)\n",
    "#print('Ranks of y_train\\n:', y_train)\n",
    "#print('-----------------------------------')\n",
    "#print('LR classifier Training set predictions:\\n',y_pred_train_LR)\n",
    "#print('Training set accuracy:', accuracy_score(y_train, y_pred_train_LR))    \n",
    "y_pred_test_LR = LR.predict(X_test_tf)\n",
    "#print('Ranks of y_test\\n:', y_test)\n",
    "#print('-----------------------------------')\n",
    "#print('LR classifier Test set predictions:\\n',y_pred_test_LR)\n",
    "LR_score = accuracy_score(y_test, y_pred_test_LR)\n",
    "#print('Test set accuracy:', np.round(LR_score,3))\n",
    "\n",
    "# Decision tree classifier\n",
    "DT= DecisionTreeClassifier()\n",
    "DT.fit(X_train_tf, y_train) \n",
    "    \n",
    "#predicted train data\n",
    "y_pred_train_DT = DT.predict(X_train_tf)\n",
    "#print('Ranks of y_train\\n:', y_train)\n",
    "#print('-----------------------------------')\n",
    "#print('DT classifier Training set predictions:\\n',y_pred_train_DT)\n",
    "#print('Training set accuracy:', accuracy_score(y_train, y_pred_train_DT))   \n",
    "\n",
    "y_pred_test_DT = DT.predict(X_test_tf)\n",
    "#print('Ranks of y_test\\n:', y_test)\n",
    "#print('-----------------------------------')\n",
    "#print('DT classifier Test set predictions:\\n',y_pred_test_DT)\n",
    "DT_score = accuracy_score(y_test, y_pred_test_DT)\n",
    "#print('Test set accuracy:', np.round(DT_score,3))\n",
    "\n",
    "# SGDclassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "SGD = SGDClassifier(random_state=123)\n",
    "SGD_clf_scores = cross_val_score(SGD, X_train_tf, y_train, cv=3)\n",
    "#print('SGD cross val scores:', SGD_clf_scores)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (SGD_clf_scores.mean(), SGD_clf_scores.std() * 2))\n",
    "SGD.fit(X_train_tf, y_train)\n",
    "#predicted train data\n",
    "y_pred_train_SGD = SGD.predict(X_train_tf)\n",
    "#print('Ranks of y_train\\n:', y_train)\n",
    "#print('-----------------------------------')\n",
    "#print('SGD classifier Training set predictions:\\n', y_pred_train_SGD)\n",
    "#print('SGD training set accuracy:', accuracy_score(y_train, y_pred_train_SGD))\n",
    "y_pred_test_SGD = SGD.predict(X_test_tf)\n",
    "#print('Ranks of y_test\\n:', y_test)\n",
    "#print('-----------------------------------')\n",
    "#print('SGD classifier Test set predictions:\\n',y_pred_test_SGD)\n",
    "SGD_score = accuracy_score(y_test, y_pred_test_SGD)\n",
    "#print('SGD Test set accuracy:', np.round(SGD_score,3))\n",
    "\n",
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train_tf, y_train)\n",
    "#predicted train data\n",
    "y_pred_train_RF = RF.predict(X_train_tf)\n",
    "#print('Ranks of y_train\\n:', y_train)\n",
    "#print('RF classifier Training set predictions:\\n',y_pred_train_RF)\n",
    "#print('RF training set accuracy:', accuracy_score(y_train, y_pred_train_RF))\n",
    "#print('-----------------------------------')\n",
    "y_pred_test_RF = RF.predict(X_test_tf)\n",
    "#print('Ranks of y_test\\n:', y_test)\n",
    "#print('RF classifier Test set predictions:\\n', y_pred_test_RF)\n",
    "RF_score = accuracy_score(y_test, y_pred_test_RF)\n",
    "#print('RF Test set accuracy:', np.round(RF_score,3))\n",
    "#print('-----------------------------------')\n",
    "\n",
    "pickle.dump(DT, open('model.pkl','wb'))\n",
    "\n",
    "# Predict the class of ALL new/unseen Abstracts from Azure blob/container\n",
    "\n",
    "# Access .json file from Azure container\n",
    "# keywords = sys.argv[1]\n",
    "keywords = ''\n",
    "\n",
    "def access_json(keywords):\n",
    "# function access .json file/Azure blob from Azure container using keywords\n",
    "    import os\n",
    "    from azure.storage.blob import BlobServiceClient\n",
    "    from smart_open import open\n",
    "    conn_str=\"DefaultEndpointsProtocol=https;AccountName=aimldatastore;AccountKey=hD7colWyUVASuZ2FOCZRudrPazQy7UOIEKhNkcsIu5q6cw9QKWDxMZczGkue8tCSXi0yAgxxbyXA+ASt9egFYw==;EndpointSuffix=core.windows.net\"\n",
    "    t_params = {\n",
    "        'client': BlobServiceClient.from_connection_string(conn_str),\n",
    "    }\n",
    "    # file from Azure Blob Storage\n",
    "    #print('python version:', sys.version)\n",
    "    #print('pandas verion:', pd.__version__)\n",
    "    # Try opening the file from azure. If that doesn't work, use the local json file\n",
    "    try: # to open the file on azure\n",
    "        f = open('azure://literaturemining/'+keywords+'.json', transport_params=t_params, encoding='utf-8')\n",
    "        new_data = pd.read_json(f)\n",
    "        f.close()\n",
    "        print(\"We successfully opened the json file on azure\")\n",
    "    except: # open the local file\n",
    "        print(\"We couldn't open the file on azure\")\n",
    "    #print(new_data.head())\n",
    "        \n",
    "    # processing the .json file\n",
    "    # remove extra spaces, missing rows, empty strings in Abstract column and reset index and lemmatize words\n",
    "    new_data['Abstract'] = new_data['Abstract'].str.strip()\n",
    "    new_data= new_data[new_data['Abstract'].notna()]\n",
    "    new_data= new_data[new_data['Abstract']!='']\n",
    "    new_data= new_data.reset_index(drop=True)\n",
    "\n",
    "    new_data['lemma_text'] = new_data['Abstract'].apply(\n",
    "        lambda x: \" \".join([lemmatizer.lemmatize(i) \n",
    "                            for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "    \n",
    "    data_set=list(new_data['lemma_text'])\n",
    "    # tranform text to vectorizer\n",
    "    test_input = vectorizer_tfidf.transform(data_set)\n",
    "    \n",
    "    # Loading model to compare the results\n",
    "    model = pickle.load(open('model.pkl','rb'))\n",
    "\n",
    "    # predict class of new abstracts with DecisionTree classifier\n",
    "    # create list of predicted labels\n",
    "    DT_rank =[]\n",
    "    for i in test_input:\n",
    "        res=model.predict(i)\n",
    "        \n",
    "        prob = model.predict_proba(i)\n",
    "        print('prob_values:', prob)\n",
    "        print(res)          \n",
    "        if res=='high':\n",
    "            DT_rank.append(\"high\")\n",
    "        elif res=='medium':\n",
    "            DT_rank.append(\"medium\")\n",
    "        else:\n",
    "            DT_rank.append(\"low\")  \n",
    "\n",
    "    # convert list of labels to dataframe\n",
    "    DT_labels=pd.DataFrame({'DT_Labels': DT_rank})\n",
    "    # concatenate dataframes\n",
    "    Abstracts_class=pd.concat([new_data, DT_labels], axis=1)\n",
    "    Abstracts_class['NLP_predicted_rank (1=high, 2=medium, 3=low)']= Abstracts_class['DT_Labels'].map({'high':1, 'medium':2, 'low':3})\n",
    "    Abstracts_class.sort_values(by='NLP_predicted_rank (1=high, 2=medium, 3=low)', inplace=True)\n",
    "    Abstracts_class = Abstracts_class.loc[:, ['Title','Abstract', 'OriginalURL','NLP_predicted_rank (1=high, 2=medium, 3=low)']]\n",
    "    #print(Abstracts_class.columns)\n",
    "    # write resutls to \n",
    "    Abstracts_class.to_excel(''+keywords+'_output.xlsx')\n",
    "\n",
    "    \n",
    "    # Write NLP output to Azure blob storage container\n",
    "    from azure.storage.blob import ContainerClient\n",
    "    blob = BlobClient.from_connection_string(\n",
    "        conn_str=\"DefaultEndpointsProtocol=https;AccountName=aimldatastore;AccountKey=hD7colWyUVASuZ2FOCZRudrPazQy7UOIEKhNkcsIu5q6cw9QKWDxMZczGkue8tCSXi0yAgxxbyXA+ASt9egFYw==;EndpointSuffix=core.windows.net\",\n",
    "        container_name=\"literaturemining\",\n",
    "        blob_name=''+keywords+'_output.xlsx')\n",
    "    with open(''+keywords+'_output.xlsx', \"rb\") as data:\n",
    "        blob.upload_blob(data, overwrite=True)\n",
    "\n",
    "access_json('Fixed_sesquiterpenes')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "48c4b789acf423f07119014e61c9c4f7b59ea82888d8551c102dbda30d7613bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
